
import json
import numpy as np
import faiss
from huggingface_hub import InferenceClient, login
import os
from transformers import AutoTokenizer, AutoModel
import torch

# -------------------------------
# 1Ô∏è‚É£ Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† token
# -------------------------------

NPY_PATH = r"D:\du_an_doc_sach_tranformer\data_xong\bach_tuyet_embeddings.npy"
JSON_PATH = r"D:\du_an_doc_sach_tranformer\data_xong\bach_tuyet_metadata.json"
HF_TOKEN = "add token here"  # token c·ªßa b·∫°n

if HF_TOKEN and HF_TOKEN.startswith("hf_"):
    login(token=HF_TOKEN)
    print("‚úÖ ƒêƒÉng nh·∫≠p Hugging Face th√†nh c√¥ng!")
else:
    print("‚ö†Ô∏è Kh√¥ng d√πng token (c√≥ th·ªÉ gi·ªõi h·∫°n rate).")

# Ki·ªÉm tra d·ªØ li·ªáu
assert os.path.exists(NPY_PATH), f"Kh√¥ng t√¨m th·∫•y: {NPY_PATH}"
assert os.path.exists(JSON_PATH), f"Kh√¥ng t√¨m th·∫•y: {JSON_PATH}"
print("‚úÖ D·ªØ li·ªáu s·∫µn s√†ng!")

# -------------------------------
# 2Ô∏è‚É£ Load embeddings & metadata
# -------------------------------
print("üîπ ƒêang load embeddings & metadata...")
embeddings = np.load(NPY_PATH).astype("float32")
with open(JSON_PATH, "r", encoding="utf-8") as f:
    metadata = json.load(f)

metadata = [m for m in metadata if "error" not in m]
min_len = min(len(embeddings), len(metadata))
embeddings, metadata = embeddings[:min_len], metadata[:min_len]

# FAISS index
d = embeddings.shape[1]
index = faiss.IndexFlatIP(d)
faiss.normalize_L2(embeddings)
index.add(embeddings)
print(f"‚úÖ FAISS index: {index.ntotal} vectors (dim={d})")

# -------------------------------
# 3Ô∏è‚É£ Model sinh (API) v√† model embed (LOCAL - FIX)
# -------------------------------
GEN_MODEL = "meta-llama/Llama-3.2-3B-Instruct"
EMBED_MODEL = "intfloat/multilingual-e5-large"  # Gi·ªØ, nh∆∞ng load local

gen_client = InferenceClient(model=GEN_MODEL, token=HF_TOKEN)

# FIX: Load LOCAL cho embed (t·∫£i ~500MB l·∫ßn ƒë·∫ßu, cache sau)
CACHE_DIR = r"D:\du_an_doc_sach_tranformer\file_model"
embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL, token=HF_TOKEN, cache_dir=CACHE_DIR)
embed_model = AutoModel.from_pretrained(
    EMBED_MODEL,
    token=HF_TOKEN,
    cache_dir=CACHE_DIR,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
embed_model.eval()
print(f"‚úÖ Model sinh (API): {GEN_MODEL}")
print(f"‚úÖ Model embed (local): {EMBED_MODEL} tr√™n {'GPU' if torch.cuda.is_available() else 'CPU'}")

# -------------------------------
# 4Ô∏è‚É£ H√†m encode query + FAISS search (FIXED - Local)
# -------------------------------
def encode_query_local(text):
    """Encode query LOCAL (mean pooling chu·∫©n + normalize L2)"""
    try:
        inputs = embed_tokenizer(
            f"query: {text}",
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(embed_model.device)

        with torch.no_grad():
            outputs = embed_model(**inputs)
            last_hidden = outputs.last_hidden_state
            attention_mask = inputs['attention_mask'].unsqueeze(-1)
            vec = (last_hidden * attention_mask).sum(1) / attention_mask.sum(1)
            vec = vec.cpu().numpy().astype("float32")

        faiss.normalize_L2(vec)
        print("üß© Query embedding shape:", vec.shape)
        return vec
    except Exception as e:
        print("‚ö†Ô∏è L·ªói encode_query_local:", e)
        dummy = embeddings[0:1].copy()
        faiss.normalize_L2(dummy)
        return dummy


def search_faiss_api(query_text, k=5):
    q_emb = encode_query_local(query_text)
    if q_emb.shape[1] != d:
        print(f"‚ö†Ô∏è Dimension mismatch! Query={q_emb.shape[1]}, Index={d}")
        return []
    scores, indices = index.search(q_emb, k)
    results = []
    for sc, idx in zip(scores[0], indices[0]):
        idx = int(idx)
        if 0 <= idx < len(metadata):
            results.append({
                "score": float(sc),
                "chunk_idx": metadata[idx].get("chunk_idx", idx),
                "text": metadata[idx]["text"]
            })
    return results

# -------------------------------
# 5Ô∏è‚É£ Build prompt v√† sinh text
# -------------------------------
def build_messages(question, retrieved):
    system = {
        "role": "system",
        "content": (
            "B·∫°n l√† tr·ª£ l√Ω AI ti·∫øng Vi·ªát. "
            "Ch·ªâ d·ª±a v√†o c√°c ƒëo·∫°n vƒÉn d∆∞·ªõi ƒë·ªÉ tr·∫£ l·ªùi. "
            "N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i 'Kh√¥ng t√¨m th·∫•y trong t√†i li·ªáu.'"
        )
    }
    passages = []
    for i, r in enumerate(retrieved):
        passages.append(f"[{i}] (score={r['score']:.3f}):\n{r['text'][:800]}")
    user = {
        "role": "user",
        "content": (
            "C√°c ƒëo·∫°n vƒÉn:\n\n" +
            "\n\n".join(passages) +
            f"\n\nC√¢u h·ªèi: {question}\n\nTr·∫£ l·ªùi t·ª± nhi√™n, chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát."
        )
    }
    return [system, user]

def generate_with_api(messages, max_new_tokens=256):
    try:
        response = gen_client.chat_completion(
            messages=messages,
            max_tokens=max_new_tokens,
            temperature=0.7,
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"‚ö†Ô∏è Chat completion fail ({e})")
        return "Kh√¥ng th·ªÉ sinh c√¢u tr·∫£ l·ªùi."

# -------------------------------
# 6Ô∏è‚É£ Pipeline RAG
# -------------------------------
def rag_pipeline(question, top_k=3):
    retrieved = search_faiss_api(question, k=top_k)
    messages = build_messages(question, retrieved)
    answer = generate_with_api(messages)

    print("\n=== üîç C√°c ƒëo·∫°n vƒÉn RAG ===")
    for i, r in enumerate(retrieved):
        print(f"[{i}] score={r['score']:.4f}")
        print(r['text'][:300].replace("\n", " ") + ("..." if len(r['text']) > 300 else ""))
        print("----")

    print("\n=== üí¨ C√¢u tr·∫£ l·ªùi LLM ===")
    print(answer)
    return answer

# ============================================================
# üîç TH√äM PH·∫¶N TEST DEBUG T·ª∞ ƒê·ªòNG ·ªû ƒê√ÇY
# ============================================================

print("\n================= üîß KI·ªÇM TRA M√îI TR∆Ø·ªúNG =================")
print("DEVICE:", "cuda" if torch.cuda.is_available() else "cpu")
print("Embeddings shape:", embeddings.shape)
print("Index ntotal:", index.ntotal)
print("d (dim):", d)
print("Metadata length:", len(metadata))
if len(metadata) > 0:
    print("Metadata[0] keys:", list(metadata[0].keys()))

print("\n================= üß† TEST ENCODE QUERY LOCAL =================")
try:
    q_test = "Ki·ªÉm tra embedding c·ªßa c√¢u h·ªèi"
    vec = encode_query_local(q_test)
    print("vec.shape:", getattr(vec, "shape", None))
    print("vec (first 10 values):", vec.flatten()[:10])
    print("vec L2 norm:", np.linalg.norm(vec))

    if len(metadata) > 0:
        text0 = metadata[0]["text"]
        vec0 = encode_query_local(text0)
        scores_with_first = np.dot(embeddings[:5], vec0.flatten())
        print("Dot scores with first 5 embeddings:", scores_with_first)
except Exception as e:
    print("‚ùå Encode test failed:", e)

print("\n================= üîé TEST FAISS SEARCH =================")
if len(metadata) > 0:
    q_search = metadata[0]["text"][:200]
    results = search_faiss_api(q_search, k=5)
    print("Search results (top k):")
    for i, r in enumerate(results):
        print(i, "score:", r["score"], "chunk_idx:", r["chunk_idx"])
else:
    print("‚ö†Ô∏è No metadata to test search.")

print("\n================= üí¨ TEST GENERATION (API) =================")
try:
    test_msgs = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "H√£y n√≥i m·ªôt c√¢u ti·∫øng Vi·ªát ng·∫Øn: Xin ch√†o th·∫ø gi·ªõi!"}
    ]
    resp = gen_client.chat_completion(messages=test_msgs, max_tokens=32, temperature=0.3)
    print("resp type:", type(resp))
    try:
        print("choices keys:", [c.keys() for c in resp.choices])
        print("first message:", resp.choices[0].message.content)
    except Exception as e:
        print("Cannot parse resp, dump repr:", repr(resp)[:800])
except Exception as e:
    print("‚ùå API generation test failed:", e)

print("\n================= ‚úÖ TEST PIPELINE MINI =================")
try:
    if len(metadata) > 0:
        q_demo = metadata[0]["text"][:60]
    else:
        q_demo = "N·ªôi dung th·ª≠ nghi·ªám"
    print("Query:", q_demo)
    retr = search_faiss_api(q_demo, k=3)
    print("Retrieved count:", len(retr))
    for i, r in enumerate(retr):
        print(f"[{i}] score={r['score']:.4f} len_text={len(r['text'])}")
    msgs = build_messages(q_demo, retr)
    print("SYSTEM:", msgs[0]["content"][:200])
    print("USER:", msgs[1]["content"][:400])
    ans = generate_with_api(msgs, max_new_tokens=64)
    print("Generation result:", ans)
except Exception as e:
    print("‚ùå Pipeline mini test failed:", e)

# ============================================================
# 7Ô∏è‚É£ Run interactive
# ============================================================
if __name__ == "__main__":
    print("\nüìö H·ªá th·ªëng RAG B·∫°ch Tuy·∫øt (local embed + API gen) s·∫µn s√†ng!")
    while True:
        q = input("\n‚ùì C√¢u h·ªèi (ho·∫∑c 'exit'):\n> ").strip()
        if q.lower() == "exit":
            break
        if q:
            print("\n--- üî¨ TEST ENCODE QUERY SAU INPUT ---")
            q_vec = encode_query_local(q)
            print("Shape:", q_vec.shape)
            print("First 10 values:", q_vec.flatten()[:10])
            print("L2 norm:", np.linalg.norm(q_vec))
            print("--- üî¨ END ENCODE TEST ---\n")

            rag_pipeline(q, top_k=3)
